{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e78b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # this reads .env and adds env variables\n",
    "\n",
    "api_key = os.environ.get(\"NOMIC_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"NOMIC_API_KEY not found. Check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94eb1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 871 chunks.\n",
      "Generated embeddings: 871\n"
     ]
    }
   ],
   "source": [
    "# Phase 2 — Build Embeddings + ChromaDB index using Nomic API\n",
    "\n",
    "import os\n",
    "import json\n",
    "from nomic import embed\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "CHUNKS_PATH = DATA / \"chunked_data.jsonl\"\n",
    "DB_DIR = Path(\"chroma_db\")\n",
    "\n",
    "if not CHUNKS_PATH.exists():\n",
    "    raise FileNotFoundError(\"chunked_data.jsonl not found in /data folder\")\n",
    "\n",
    "# Load chunks (JSONL)\n",
    "chunks = []\n",
    "with CHUNKS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            chunks.append(json.loads(line))\n",
    "\n",
    "print(\"Loaded\", len(chunks), \"chunks.\")\n",
    "\n",
    "# Prepare lists\n",
    "ids, texts, metas = [], [], []\n",
    "\n",
    "for c in chunks:\n",
    "    ids.append(str(c[\"chunk_id\"]))\n",
    "    texts.append(c[\"text\"].strip())\n",
    "    metas.append({\n",
    "        \"chunk_id\": c.get(\"chunk_id\"),\n",
    "        \"chunk_type\": c.get(\"chunk_type\"),\n",
    "        \"act\": c.get(\"act\"),\n",
    "        \"scene\": c.get(\"scene\"),\n",
    "        \"speaker\": c.get(\"speaker\"),\n",
    "        \"is_soliloquy\": c.get(\"is_soliloquy\"),\n",
    "        \"start_page\": c.get(\"start_page\"),\n",
    "        \"end_page\": c.get(\"end_page\")\n",
    "    })\n",
    "\n",
    "# Generate embeddings\n",
    "BATCH_SIZE = 64\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(texts), BATCH_SIZE):\n",
    "    batch = texts[i:i + BATCH_SIZE]\n",
    "    resp = embed.text(\n",
    "        texts=batch,\n",
    "        model=\"nomic-embed-text-v1.5\"\n",
    "    )\n",
    "    embeddings.extend(resp[\"embeddings\"])\n",
    "\n",
    "print(\"Generated embeddings:\", len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd872097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks: 871\n",
      "FAISS index created and saved at: faiss_index\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Phase 2 — FAISS Indexing using Nomic Embeddings\n",
    "# Clean, stable, no Chroma dependency issues\n",
    "# ============================================\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from nomic import embed\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "DATA = Path(\"data\")\n",
    "CHUNKS_PATH = DATA / \"chunked_data.jsonl\"\n",
    "FAISS_DIR = \"faiss_index\"\n",
    "\n",
    "if not CHUNKS_PATH.exists():\n",
    "    raise FileNotFoundError(\"chunked_data.jsonl not found\")\n",
    "\n",
    "if not (os.environ.get(\"NOMIC_API_KEY\") or Path.home().joinpath(\".nomic\").exists()):\n",
    "    raise ValueError(\"Nomic API not configured\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Custom Nomic Embedding Adapter\n",
    "# ============================\n",
    "class NomicEmbeddings(Embeddings):\n",
    "    def __init__(self, model=\"nomic-embed-text-v1.5\", batch_size=64):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i+self.batch_size]\n",
    "            resp = embed.text(texts=batch, model=self.model)\n",
    "            embeddings.extend(resp[\"embeddings\"])\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        resp = embed.text(texts=[text], model=self.model)\n",
    "        return resp[\"embeddings\"][0]\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Load Chunks (JSONL)\n",
    "# ============================\n",
    "texts = []\n",
    "metas = []\n",
    "ids = []\n",
    "\n",
    "with CHUNKS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        rec = json.loads(line)\n",
    "        ids.append(str(rec.get(\"chunk_id\")))\n",
    "        texts.append((rec.get(\"text\") or \"\").strip())\n",
    "        metas.append({\n",
    "            \"chunk_id\": rec.get(\"chunk_id\"),\n",
    "            \"chunk_type\": rec.get(\"chunk_type\"),\n",
    "            \"act\": rec.get(\"act\"),\n",
    "            \"scene\": rec.get(\"scene\"),\n",
    "            \"speaker\": rec.get(\"speaker\"),\n",
    "            \"is_soliloquy\": rec.get(\"is_soliloquy\"),\n",
    "            \"start_page\": rec.get(\"start_page\"),\n",
    "            \"end_page\": rec.get(\"end_page\"),\n",
    "        })\n",
    "\n",
    "print(\"Loaded chunks:\", len(texts))\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Create FAISS Vector Store\n",
    "# ============================\n",
    "embedding_fn = NomicEmbeddings()\n",
    "\n",
    "faiss_store = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_fn,\n",
    "    metadatas=metas,\n",
    ")\n",
    "\n",
    "# Save FAISS index to disk\n",
    "faiss_store.save_local(FAISS_DIR)\n",
    "\n",
    "print(\"FAISS index created and saved at:\", FAISS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
